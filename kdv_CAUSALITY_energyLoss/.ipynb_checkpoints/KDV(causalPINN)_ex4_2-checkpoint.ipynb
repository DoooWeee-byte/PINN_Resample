{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\" compute the derivative of outputs associated with inputs\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "    outputs: (N, 1) tensor\n",
    "    inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                               grad_outputs=torch.ones_like(outputs),\n",
    "                               create_graph=True)\n",
    "\n",
    "def activation(name):\n",
    "    if name in ['tanh', 'Tanh']:\n",
    "        return nn.Tanh()\n",
    "    elif name in ['relu', 'ReLU']:\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif name in ['leaky_relu', 'LeakyReLU']:\n",
    "        return nn.LeakyReLU(inplace=True)\n",
    "    elif name in ['sigmoid', 'Sigmoid']:\n",
    "        return nn.Sigmoid()\n",
    "    elif name in ['softplus', 'Softplus']:\n",
    "        return nn.Softplus()\n",
    "    else:\n",
    "        raise ValueError(f'unknown activation function: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Deep Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self, L, M, dim_hidden, hidden_layers, dim_out,\n",
    "                 act_name='tanh', init_name='xavier_normal'):\n",
    "        super().__init__()\n",
    "        \n",
    "        dim_in = M * 2 + 2\n",
    "        \n",
    "        model = nn.Sequential()\n",
    "        \n",
    "        model.add_module('fc0', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        model.add_module('act0', activation(act_name))\n",
    "        for i in range(1, hidden_layers):\n",
    "            model.add_module(f'fc{i}', nn.Linear(dim_hidden, dim_hidden, bias=True))\n",
    "            model.add_module(f'act{i}', activation(act_name))\n",
    "        model.add_module(f'fc{hidden_layers}', nn.Linear(dim_hidden, dim_out, bias=True))\n",
    "            \n",
    "        self.model = model\n",
    "        \n",
    "        self.L = L\n",
    "        self.M = M\n",
    "        \n",
    "        if init_name is not None:\n",
    "            self.init_weight(init_name)\n",
    "\n",
    "            \n",
    "        self.k = nn.Parameter(torch.arange(1, self.M+1), requires_grad=False)\n",
    "                    \n",
    "    def init_weight(self, name):\n",
    "        if name == 'xavier_normal':\n",
    "            nn_init = nn.init.xavier_normal_\n",
    "        elif name == 'xavier_uniform':\n",
    "            nn_init = nn.init.xavier_uniform_\n",
    "        elif name == 'kaiming_normal':\n",
    "            nn_init = nn.init.kaiming_normal_\n",
    "        elif name == 'kaiming_uniform':\n",
    "            nn_init = nn.init.kaiming_uniform_\n",
    "        else:\n",
    "            raise ValueError(f'unknown initialization function: {name}')\n",
    "\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn_init(param)\n",
    "                \n",
    "#         for layer, param in enumerate(self.parameters()):\n",
    "#             if layer % 2 == 1:\n",
    "#                 nn.init.constant_(param, 0.0)\n",
    "#             else:\n",
    "#                 nn_init(param)\n",
    "                \n",
    "    def input_encoding(self, t, x):\n",
    "        w = 2.0 * math.pi / self.L\n",
    "        out = torch.cat([t, torch.ones_like(t), \n",
    "                            torch.cos(self.k * w * x), torch.sin(self.k * w * x)], dim = 1) \n",
    "        \n",
    "        return out    \n",
    "            \n",
    "    def forward(self, H):\n",
    "        t = H[:, 0:1]\n",
    "        x = H[:, 1:2]\n",
    "        \n",
    "        H = self.input_encoding(t, x)\n",
    "        H = self.model(H)\n",
    "        \n",
    "        return H\n",
    "    \n",
    "    def forward_test(self, x):\n",
    "        print(f\"{'input':<20}{str(x.shape):<40}\")\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            print(f\"{name:<20}{str(x.shape):<40}\")\n",
    "        return x\n",
    "\n",
    "    def model_size(self):\n",
    "        n_params = 0\n",
    "        for param in self.parameters():\n",
    "            n_params += param.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(L=2.0, M=10, dim_hidden=128, hidden_layers=4, dim_out=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDE 部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KDV方程：\n",
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    "&u_{t}+ \\lambda_1 u u_{x} + \\lambda_2 u_{xxx}=0  ,(t,x)\\in (0,1)\\times (-1,1),  \\\\\n",
    "&u(t,-1)=u(t,1),t\\in [0,1],  \\\\\n",
    "&u_x(t,-1)=u_x(t,1),x\\in [0,1].\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "$\\lambda_1 = 1$, $\\lambda_2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "真解设置为:\n",
    "$$u(x, t) = 12  \\frac{ k_1^2 e^{\\theta_1}  + k_2^2 e^{\\theta_2} + 2 (k_2 - k_1)^2 e^{\\theta_1 + \\theta_2} + a^2 ( k_2^2 e^{\\theta_1} + k_1^2 e^{\\theta_2})e^{\\theta_1 + \\theta_2}}{(1 + e^{\\theta_1} + e^{\\theta_2} + a^2 e^{\\theta_1 + \\theta_2})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ k_1 = 0.4$, $k_2 = 0.6$, $a^2 = \\left (  \\frac{ k_1 - k_2 }{k_1 + k_2 } \\right )^2 = \\frac{1}{25}$, \n",
    "\n",
    "\n",
    "$\\theta_1 = k_1 x - k_1^3 t + x_1 $, $\\theta_2 = k_2 x - k_2^3 t + x_2$, \n",
    "\n",
    "\n",
    "$ x_1 = 4$, $x_2 = 15$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "class Options_KDV(object):\n",
    "    def __init__(self):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--no_cuda', action='store_true', default=False, help='disable CUDA or not')\n",
    "        parser.add_argument('--dim_hidden', type=int, default=12, help='neurons in hidden layers')     # 10 9\n",
    "        parser.add_argument('--hidden_layers', type=int, default=9, help='number of hidden layers')    # 4  20\n",
    "        parser.add_argument('--lam', type=float, default=1, help='weight in loss function')\n",
    "        parser.add_argument('--lr', type=float, default=0.001, help='initial learning rate')\n",
    "        parser.add_argument('--epochs_Adam', type=int, default=600000, help='epochs for Adam optimizer')\n",
    "        parser.add_argument('--epochs_LBFGS', type=int, default=2500, help='epochs for LBFGS optimizer')\n",
    "        parser.add_argument('--newton_iter', type=int, default=100, help='newton_iter for LBFGS optimizer')\n",
    "        parser.add_argument('--step_size', type=int, default=10000, help='step size in lr_scheduler for Adam optimizer')\n",
    "        parser.add_argument('--gamma', type=float, default=0.9, help='gamma in lr_scheduler for Adam optimizer')\n",
    "        parser.add_argument('--tol', type=float, default=100, help='the annealing scheme')\n",
    "        parser.add_argument('--resume', type=bool, default=False, help='resume or not')\n",
    "        parser.add_argument('--sample_method', type=str, default='uniform', help='sample method')\n",
    "\n",
    "        self.parser = parser\n",
    "\n",
    "    def parse(self):\n",
    "        arg = self.parser.parse_args(args=[])\n",
    "        arg.load_model = False\n",
    "        arg.cuda = not arg.no_cuda and torch.cuda.is_available()\n",
    "        # arg.cuda = False\n",
    "        arg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(arg.cuda)\n",
    "        print(arg.device)\n",
    "        return arg\n",
    "\n",
    "args = Options_KDV().parse()\n",
    "print(args.hidden_layers)\n",
    "\n",
    "def save_model(state, is_best=None, save_dir=None):\n",
    "    last_model = os.path.join(save_dir, 'last_model.pth.tar')\n",
    "    torch.save(state, last_model)\n",
    "    if is_best:\n",
    "        best_model = os.path.join(save_dir, 'best_model.pth.tar')\n",
    "        shutil.copyfile(last_model, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "args.model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真解\n",
    "def u(X):\n",
    "    if X.ndim == 2 and X.shape[1] != 1:\n",
    "        x = X[:, 1]\n",
    "        t = X[:, 0]\n",
    "    else :\n",
    "        x = X\n",
    "        t = np.zeros_like(x)\n",
    "    k1, k2 = 0.4, 0.6\n",
    "    a2 = 1./25\n",
    "    x1, x2 = 4, 15\n",
    "    theta1 = k1*x - k1**3 * t + x1\n",
    "    theta2 = k2*x - k2**3 * t + x2\n",
    "    up = 12*(k1 ** 2 * np.exp(theta1) + \n",
    "           k2 ** 2 * np.exp(theta2) + \n",
    "           2 * (k2 - k1) **2 * np.exp(theta1 + theta2)\n",
    "           + a2 * (k2 ** 2 * np.exp(theta1) + k1**2 * np.exp(theta2)) * np.exp(theta1 + theta2))\n",
    "    down = (1 + np.exp(theta1) + np.exp(theta2) + a2 * np.exp(theta1 + theta2)) ** 2\n",
    "    u = up/down\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainset_KDV():\n",
    "    '''\n",
    "    时间上[0, 120] 划分1200段\n",
    "    空间上[-40, 40] 取出4000个高斯点\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        self.shape = (self.args[0], self.args[1])\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.data()\n",
    "    \n",
    "    def data(self):\n",
    "        # 为了生成高斯积分点，n_x一定要是4的倍数\n",
    "        gp = np.array([0.8611363116,-0.8611363116,0.3399810436,-0.3399810436])\n",
    "        gc = np.array([0.3478548451,0.3478548451,0.6521451549,0.6521451549])\n",
    "        n_t = self.args[0]\n",
    "        n_x = self.args[1]\n",
    "        n_ics = self.args[2]\n",
    "        \n",
    "        # 生成初值训练点\n",
    "        t = np.linspace(0, 120, n_t)\n",
    "        x = np.linspace(-40, 40, n_x)\n",
    "        x, t = np.meshgrid(x, t)\n",
    "        tx = np.hstack((t.reshape(-1,1), x.reshape(-1,1)))\n",
    "\n",
    "        t_ics = np.zeros(n_ics)\n",
    "        x_ics = np.linspace(-1, 1, n_ics)\n",
    "        tx_ics = np.hstack([t_ics.reshape(-1,1),x_ics.reshape(-1,1)])\n",
    "        \n",
    "        u_ics = u(tx_ics)\n",
    "        u_ics = u_ics.reshape(-1,1)\n",
    "        M = np.triu(np.ones([n_t, n_t]),k=1).T\n",
    "        \n",
    "        # 生成tgx(高斯训练点)\n",
    "        num_cell = int(n_x/4 + 1)\n",
    "        l = np.linspace(-1, 1, num_cell)[:, None]\n",
    "        l = np.hstack([l[:-1], l[1:]])\n",
    "        c = (l[:, 1] - l[:, 0])/2\n",
    "        c = c[:, None]\n",
    "        gp = gp[None, :]\n",
    "        d = ((l[:, 1] + l[:, 0])/2)\n",
    "        d = d[:, None]\n",
    "        n_p = c * gp + d  \n",
    "        n_p = n_p.reshape(n_x, 1)\n",
    "        gcl = c * gc      \n",
    "        gcl = gcl.reshape(n_x, 1)\n",
    "        t = np.linspace(0, 1, n_t)[:, None]\n",
    "        x, t = np.meshgrid(n_p, t) \n",
    "        print(x.shape)\n",
    "        txg = np.hstack([t.reshape(-1, 1), x.reshape(-1, 1)])\n",
    "        # 计算一下初始能量\n",
    "        E2 = u_ics ** 2 * gcl\n",
    "        E2 = np.sum(E2)\n",
    "        print(E2)\n",
    "        \n",
    "        tx = torch.from_numpy(tx).float().to(device)\n",
    "        txg = torch.from_numpy(txg).float().to(device)\n",
    "        \n",
    "        tx_ics = torch.from_numpy(tx_ics).float().to(device)\n",
    "        u_ics = torch.from_numpy(u_ics).float().to(device)\n",
    "        M = torch.from_numpy(M).float().to(device)\n",
    "        \n",
    "        return tx, tx_ics, u_ics, M, gcl, txg, E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 1200\n",
    "nx = 4000\n",
    "n_ics = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 4000)\n",
      "0.3393000694887256\n"
     ]
    }
   ],
   "source": [
    "# tgx是高斯点的数据集， shape :(12800, 2) 第一列代表时间，第二列代表空间 ，前100个点表示t = 0 时刻上的采样点，第101个到200表示t = 1/nt 时刻的采样点， 依次类推\n",
    "trainset = Trainset_KDV(nt, nx, n_ics)\n",
    "args.trainset = trainset\n",
    "tx, tx_ics, u_ics, M, gcl, txg, E2 = trainset()  # E2 是初始时刻的平方积分\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer_Wave(object):\n",
    "    def __init__(self, args):\n",
    "        self.model = args.model\n",
    "        self.lr = args.lr\n",
    "        self.gamma = args.gamma\n",
    "        self.trainset = args.trainset\n",
    "        self.step_size = args.step_size\n",
    "        self.model_name = self.model.__class__.__name__     \n",
    "        self.optimizer_Adam = optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999))\n",
    "        self.scheduler = lr_scheduler.ExponentialLR(self.optimizer_Adam, gamma=self.gamma)\n",
    "        self.epochs_Adam = args.epochs_Adam\n",
    "        self.tol = args.tol\n",
    "        \n",
    "        # data\n",
    "        self.tx, self.tx_ics, self.u_ics, self.M,  self.gcl, self.txg, self.E2 = self.trainset()\n",
    "        self.gcl = torch.from_numpy(self.gcl).float().to(device)\n",
    "        self.E2 = torch.tensor(self.E2).float().to(device)\n",
    "        \n",
    "        # Logger\n",
    "        self.loss_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_res_log = []\n",
    "        self.W_log = []\n",
    "        self.L_t_log = []\n",
    "        self.epoch_log = []\n",
    "        \n",
    "\n",
    "    def net_r(self, tx):\n",
    "        tx.requires_grad_(True).to(device)\n",
    "        u = self.model(tx)\n",
    "        grad_u = grad(u, tx)[0]\n",
    "        u_t = grad_u[:,[0]]\n",
    "        u_x = grad_u[:,[1]]\n",
    "        u_xx = grad(u_x, tx)[0][:, [1]]\n",
    "        u_xxx = grad(u_xx, tx)[0][:, [1]]\n",
    "\n",
    "        residual = u_t  + u * u_x + 0.0025 * u_xxx\n",
    "\n",
    "        return residual\n",
    "\n",
    "    def net_u(self, tx):\n",
    "        u = self.model(tx)  \n",
    "        return u\n",
    "    \n",
    "    def residuals_and_weights(self):\n",
    "        r_pred = self.net_r(self.txg) \n",
    "        r_pred = r_pred.reshape(100, 128) # :shape(100,100)\n",
    "        \n",
    "        q_pred = self.net_u(self.txg)\n",
    "        q_pred = q_pred.reshape(100, 128)\n",
    "        #print(q_pred.shape)\n",
    "        q_pred = q_pred ** 2 * self.gcl\n",
    "        q_pred = torch.sum(q_pred, axis=0)\n",
    "        q_pred = (q_pred - self.E2) ** 2\n",
    "        #print(q_pred.shape)\n",
    "        #print(\"q:\",torch.sum(q_pred))\n",
    "        #print(q_pred.shape, min(q_pred). max(q_pred))\n",
    "        L_t = torch.mean(r_pred**2, axis=0) \n",
    "        L_t = L_t + 10 * q_pred\n",
    "        \n",
    "        W = torch.exp(-self.tol * (self.M @ L_t.detach()))\n",
    "        \n",
    "        return L_t, W\n",
    "    \n",
    "    def loss_ics(self):\n",
    "        u_pred = self.net_u(self.tx_ics)\n",
    "        loss_ics = torch.mean((u_pred - self.u_ics)**2)\n",
    "        return loss_ics\n",
    "    \n",
    "    def loss_res(self):\n",
    "        r_pred = self.net_r(self.tx)\n",
    "        loss_r = torch.mean(r_pred**2)\n",
    "        return loss_r\n",
    "    \n",
    "    def loss(self):\n",
    "        L0 = 100 * self.loss_ics()\n",
    "        L_t, W = self.residuals_and_weights()\n",
    "        loss = torch.mean(W * L_t) + L0        \n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        start = time.time()\n",
    "        \n",
    "        for epoch in range(self.epochs_Adam):\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss_value = self.loss()\n",
    "            loss_value.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            \n",
    "            if (epoch+1) % self.step_size == 0:\n",
    "                self.scheduler.step()            \n",
    "            \n",
    "            if epoch % 1000 == 0:\n",
    "                loss_value = self.loss()\n",
    "                loss_ics_value = self.loss_ics()\n",
    "                loss_res_value = self.loss_res()\n",
    "                \n",
    "                L_t_value, W_value = self.residuals_and_weights()\n",
    "                \n",
    "                self.loss_log.append(loss_value.detach().cpu())\n",
    "                self.loss_ics_log.append(loss_ics_value.detach().cpu())\n",
    "                self.loss_res_log.append(loss_res_value.detach().cpu())\n",
    "                self.W_log.append(W_value.detach().cpu())\n",
    "                self.L_t_log.append(L_t_value.detach().cpu())\n",
    "                self.epoch_log.append(epoch)\n",
    "                \n",
    "                end = time.time()\n",
    "                running_time = end - start\n",
    "                start = time.time()\n",
    "                \n",
    "                print(f'Epoch #  {epoch}/{self.epochs_Adam}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                      f'loss:{loss_value:.2e}, loss_ics:{loss_ics_value:.2e}, loss_res:{loss_res_value:.2e},')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset.shape (400, 1200)\n",
      "(400, 1200) (400, 1200)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 236.00 MiB (GPU 0; 10.76 GiB total capacity; 3.02 GiB already allocated; 71.56 MiB free; 3.06 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b7ab6d38084a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer_Wave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-db0c1ccb5b61>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs_Adam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_Adam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_Adam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-db0c1ccb5b61>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mL0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_ics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mL_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduals_and_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mL_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mL0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-db0c1ccb5b61>\u001b[0m in \u001b[0;36mresiduals_and_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresiduals_and_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mr_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# :shape(100,100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mq_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-db0c1ccb5b61>\u001b[0m in \u001b[0;36mnet_r\u001b[0;34m(self, tx)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mgrad_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pde/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-57bb149e7551>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, H)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pde/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pde/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pde/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pde/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pde/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 236.00 MiB (GPU 0; 10.76 GiB total capacity; 3.02 GiB already allocated; 71.56 MiB free; 3.06 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer = Trainer_Wave(args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
