{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "\n",
    "def grad(outputs, inputs):\n",
    "    \"\"\" compute the derivative of outputs associated with inputs\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "    outputs: (N, 1) tensor\n",
    "    inputs: (N, D) tensor\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs,\n",
    "                               grad_outputs=torch.ones_like(outputs),\n",
    "                               create_graph=True)\n",
    "\n",
    "def activation(name):\n",
    "    if name in ['tanh', 'Tanh']:\n",
    "        return nn.Tanh()\n",
    "    elif name in ['relu', 'ReLU']:\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif name in ['leaky_relu', 'LeakyReLU']:\n",
    "        return nn.LeakyReLU(inplace=True)\n",
    "    elif name in ['sigmoid', 'Sigmoid']:\n",
    "        return nn.Sigmoid()\n",
    "    elif name in ['softplus', 'Softplus']:\n",
    "        return nn.Softplus()\n",
    "    else:\n",
    "        raise ValueError(f'unknown activation function: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Deep Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self, L, M, dim_hidden, hidden_layers, dim_out,\n",
    "                 act_name='tanh', init_name='xavier_normal'):\n",
    "        super().__init__()\n",
    "        \n",
    "        dim_in = M * 2 + 2\n",
    "        \n",
    "        model = nn.Sequential()\n",
    "        \n",
    "        model.add_module('fc0', nn.Linear(dim_in, dim_hidden, bias=True))\n",
    "        model.add_module('act0', activation(act_name))\n",
    "        for i in range(1, hidden_layers):\n",
    "            model.add_module(f'fc{i}', nn.Linear(dim_hidden, dim_hidden, bias=True))\n",
    "            model.add_module(f'act{i}', activation(act_name))\n",
    "        model.add_module(f'fc{hidden_layers}', nn.Linear(dim_hidden, dim_out, bias=True))\n",
    "            \n",
    "        self.model = model\n",
    "        \n",
    "        self.L = L\n",
    "        self.M = M\n",
    "        \n",
    "        if init_name is not None:\n",
    "            self.init_weight(init_name)\n",
    "\n",
    "            \n",
    "        self.k = nn.Parameter(torch.arange(1, self.M+1), requires_grad=False)\n",
    "                    \n",
    "    def init_weight(self, name):\n",
    "        if name == 'xavier_normal':\n",
    "            nn_init = nn.init.xavier_normal_\n",
    "        elif name == 'xavier_uniform':\n",
    "            nn_init = nn.init.xavier_uniform_\n",
    "        elif name == 'kaiming_normal':\n",
    "            nn_init = nn.init.kaiming_normal_\n",
    "        elif name == 'kaiming_uniform':\n",
    "            nn_init = nn.init.kaiming_uniform_\n",
    "        else:\n",
    "            raise ValueError(f'unknown initialization function: {name}')\n",
    "\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:\n",
    "                nn_init(param)\n",
    "                \n",
    "#         for layer, param in enumerate(self.parameters()):\n",
    "#             if layer % 2 == 1:\n",
    "#                 nn.init.constant_(param, 0.0)\n",
    "#             else:\n",
    "#                 nn_init(param)\n",
    "                \n",
    "    def input_encoding(self, t, x):\n",
    "        w = 2.0 * math.pi / self.L\n",
    "        out = torch.cat([t, torch.ones_like(t), \n",
    "                            torch.cos(self.k * w * x), torch.sin(self.k * w * x)], dim = 1) \n",
    "        \n",
    "        return out    \n",
    "            \n",
    "    def forward(self, H):\n",
    "        t = H[:, 0:1]\n",
    "        x = H[:, 1:2]\n",
    "        \n",
    "        H = self.input_encoding(t, x)\n",
    "        H = self.model(H)\n",
    "        \n",
    "        return H\n",
    "    \n",
    "    def forward_test(self, x):\n",
    "        print(f\"{'input':<20}{str(x.shape):<40}\")\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            print(f\"{name:<20}{str(x.shape):<40}\")\n",
    "        return x\n",
    "\n",
    "    def model_size(self):\n",
    "        n_params = 0\n",
    "        for param in self.parameters():\n",
    "            n_params += param.numel()\n",
    "        return n_params\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(L=2.0, M=10, dim_hidden=128, hidden_layers=4, dim_out=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KDV方程：\n",
    "$$\n",
    "\\left\\{\\begin{matrix}\n",
    "&u_{t}+ \\lambda_1 u u_{x} + \\lambda_2 u_{xxx}=0  ,(t,x)\\in (0,1)\\times (-1,1),  \\\\\n",
    "&u(0,x)= cos(\\pi x),  \\\\\n",
    "&u(t,-1)=u(t,1),t\\in [0,1],  \\\\\n",
    "&u_x(t,-1)=u_x(t,1),x\\in [0,1].\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "$\\lambda_1 = 1.000$, $\\lambda_2 = 0.0025$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "class Options_KDV(object):\n",
    "    def __init__(self):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--no_cuda', action='store_true', default=False, help='disable CUDA or not')\n",
    "        parser.add_argument('--dim_hidden', type=int, default=12, help='neurons in hidden layers')     # 10 9\n",
    "        parser.add_argument('--hidden_layers', type=int, default=9, help='number of hidden layers')    # 4  20\n",
    "        parser.add_argument('--lam', type=float, default=1, help='weight in loss function')\n",
    "        parser.add_argument('--lr', type=float, default=0.001, help='initial learning rate')\n",
    "        parser.add_argument('--epochs_Adam', type=int, default=300000, help='epochs for Adam optimizer')\n",
    "        parser.add_argument('--epochs_LBFGS', type=int, default=2500, help='epochs for LBFGS optimizer')\n",
    "        parser.add_argument('--newton_iter', type=int, default=100, help='newton_iter for LBFGS optimizer')\n",
    "        parser.add_argument('--step_size', type=int, default=50000, help='step size in lr_scheduler for Adam optimizer')\n",
    "        parser.add_argument('--gamma', type=float, default=0.9, help='gamma in lr_scheduler for Adam optimizer')\n",
    "        parser.add_argument('--tol', type=float, default=100, help='the annealing scheme')\n",
    "        parser.add_argument('--resume', type=bool, default=False, help='resume or not')\n",
    "        parser.add_argument('--sample_method', type=str, default='uniform', help='sample method')\n",
    "\n",
    "        self.parser = parser\n",
    "\n",
    "    def parse(self):\n",
    "        arg = self.parser.parse_args(args=[])\n",
    "        arg.load_model = False\n",
    "        arg.cuda = not arg.no_cuda and torch.cuda.is_available()\n",
    "        # arg.cuda = False\n",
    "        arg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(arg.cuda)\n",
    "        print(arg.device)\n",
    "        return arg\n",
    "\n",
    "args = Options_KDV().parse()\n",
    "print(args.hidden_layers)\n",
    "\n",
    "def save_model(state, is_best=None, save_dir=None):\n",
    "    last_model = os.path.join(save_dir, 'last_model.pth.tar')\n",
    "    torch.save(state, last_model)\n",
    "    if is_best:\n",
    "        best_model = os.path.join(save_dir, 'best_model.pth.tar')\n",
    "        shutil.copyfile(last_model, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "args.model=model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainset_KDV():\n",
    "    '''\n",
    "    训练点:时间剖分 100 段， 空间上分成25段，每段上取4个高斯点\n",
    "    '''\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        self.shape = (self.args[0], self.args[1])\n",
    "        \n",
    "    def __call__(self):\n",
    "        return self.data()\n",
    "    \n",
    "    def data(self):\n",
    "        # 为了生成高斯积分点，n_x一定要是4的倍数\n",
    "        gp = np.array([0.8611363116,-0.8611363116,0.3399810436,-0.3399810436])\n",
    "        gc = np.array([0.3478548451,0.3478548451,0.6521451549,0.6521451549])\n",
    "        n_t = self.args[0]\n",
    "        n_x = self.args[1]\n",
    "        n_ics = self.args[2]\n",
    "        \n",
    "        # 生成初值训练点\n",
    "        t = np.linspace(0, 1, n_t)\n",
    "        x = np.linspace(-1, 1, n_x)\n",
    "        x, t = np.meshgrid(x, t)\n",
    "        tx = np.hstack((t.reshape(-1,1), x.reshape(-1,1)))\n",
    "\n",
    "        t_ics = np.zeros(n_ics)\n",
    "        x_ics = np.linspace(-1, 1, n_ics)\n",
    "        tx_ics = np.hstack([t_ics.reshape(-1,1),x_ics.reshape(-1,1)])\n",
    "        \n",
    "        u_ics = np.cos(math.pi*x_ics)\n",
    "        u_ics = u_ics.reshape(-1,1)\n",
    "        M = np.triu(np.ones([n_t, n_t]),k=1).T\n",
    "        \n",
    "        # 生成tgx(高斯训练点)\n",
    "        num_cell = int(n_x/4 + 1)\n",
    "        l = np.linspace(-1, 1, num_cell)[:, None]\n",
    "        l = np.hstack([l[:-1], l[1:]])\n",
    "        c = (l[:, 1] - l[:, 0])/2\n",
    "        c = c[:, None]\n",
    "        gp = gp[None, :]\n",
    "        d = ((l[:, 1] + l[:, 0])/2)\n",
    "        d = d[:, None]\n",
    "        n_p = c * gp + d  # n_p是100个高斯积分点\n",
    "        n_p = n_p.reshape(n_x, 1)\n",
    "        gcl = c * gc     # gcl是这100个高斯积分点对应的权重  \n",
    "        gcl = gcl.reshape(n_x, 1)\n",
    "        t = np.linspace(0, 1, n_t)[:, None]\n",
    "        x, t = np.meshgrid(n_p, t) #   mesh size (128, 100)\n",
    "        print(x.shape)\n",
    "        txg = np.hstack([t.reshape(-1, 1), x.reshape(-1, 1)])\n",
    "        # 计算一下初始能量\n",
    "        E2 =  np.cos(math.pi*n_p) ** 2 * gcl\n",
    "        E2 = np.sum(E2)\n",
    "        print(E2)\n",
    "        \n",
    "        tx = torch.from_numpy(tx).float().to(device)\n",
    "        txg = torch.from_numpy(txg).float().to(device)\n",
    "        \n",
    "        tx_ics = torch.from_numpy(tx_ics).float().to(device)\n",
    "        u_ics = torch.from_numpy(u_ics).float().to(device)\n",
    "        M = torch.from_numpy(M).float().to(device)\n",
    "        \n",
    "        return tx, tx_ics, u_ics, M, gcl, txg, E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个预测网格\n",
    "# load the data\n",
    "data = scipy.io.loadmat('../data/KdV.mat')\n",
    "usol = data['uu']\n",
    "\n",
    "# Grid\n",
    "t_star = data['tt'][0]\n",
    "x_star = data['x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201,)\n",
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "print(t_star.shape)\n",
    "print(x_star.shape)\n",
    "X_pred = np.hstack([np.ones((len(x_star),1)) * t_star[0 + 1], x_star.reshape(-1,1)])\n",
    "pred = np.zeros((512, 201))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把初值填在第一列\n",
    "pred[:, 0] = np.cos(math.pi * x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = t_star.shape[0]\n",
    "nx = 100\n",
    "n_ics = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 100)\n",
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "# tgx是高斯点的数据集， shape :(12800, 2) 第一列代表时间，第二列代表空间 ，前100个点表示t = 0 时刻上的采样点，第101个到200表示t = 1/nt 时刻的采样点， 依次类推\n",
    "trainset = Trainset_KDV(nt, nx, n_ics)\n",
    "args.trainset = trainset\n",
    "tx, tx_ics, u_ics, M, gcl, txg, E2 = trainset()  # E2 是初始时刻的平方积分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer_Wave(object):\n",
    "    def __init__(self, args):\n",
    "        self.model = args.model\n",
    "        self.lr = args.lr\n",
    "        self.gamma = args.gamma\n",
    "        self.trainset = args.trainset\n",
    "        self.step_size = args.step_size\n",
    "        self.model_name = self.model.__class__.__name__     \n",
    "        self.optimizer_Adam = optim.Adam(self.model.parameters(), lr=self.lr, betas=(0.9, 0.999))\n",
    "        self.scheduler = lr_scheduler.ExponentialLR(self.optimizer_Adam, gamma=self.gamma)\n",
    "        self.epochs_Adam = args.epochs_Adam\n",
    "        self.tol = args.tol\n",
    "        \n",
    "        # data\n",
    "        self.tx, self.tx_ics, self.u_ics, self.M,  self.gcl, self.txg, self.E2 = self.trainset()\n",
    "        self.gcl = torch.from_numpy(self.gcl).float().to(device)\n",
    "        self.E2 = torch.tensor(self.E2).float().to(device)\n",
    "        self.nx = nx\n",
    "        self.nt = nt\n",
    "        \n",
    "        # Logger\n",
    "        self.loss_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_res_log = []\n",
    "        self.loss_energy_log = []\n",
    "        self.epoch_log = []\n",
    "        \n",
    "        self.x_pred = x_star\n",
    "        self.t_pred = t_star\n",
    "        self.pred = pred  # (512, 201)\n",
    "\n",
    "    def net_r(self, tx):\n",
    "        tx.requires_grad_(True).to(device)\n",
    "        u = self.model(tx)\n",
    "        grad_u = grad(u, tx)[0]\n",
    "        u_t = grad_u[:,[0]]\n",
    "        u_x = grad_u[:,[1]]\n",
    "        u_xx = grad(u_x, tx)[0][:, [1]]\n",
    "        u_xxx = grad(u_xx, tx)[0][:, [1]]\n",
    "\n",
    "        residual = u_t  + u * u_x + 0.0025 * u_xxx\n",
    "\n",
    "        return residual\n",
    "\n",
    "    def net_u(self, tx):\n",
    "        u = self.model(tx)  \n",
    "        return u\n",
    "    \n",
    "    def loss_ics(self, X, u_ics):\n",
    "        u_pred = self.net_u(X)\n",
    "        loss_ics = torch.mean((u_pred - u_ics)**2)\n",
    "        return loss_ics\n",
    "    \n",
    "    def loss_res(self, X):\n",
    "        r_pred = self.net_r(X)\n",
    "        loss_r = torch.mean(r_pred**2)\n",
    "        return loss_r\n",
    "    \n",
    "    def energy_loss(self, X):\n",
    "        q_pred = self.net_u(X)\n",
    "        num_t = int(len(X)/self.nx)\n",
    "        q_pred = q_pred.reshape(num_t, 100)\n",
    "        if self.gcl.shape != (1, 100):\n",
    "            self.gcl = self.gcl.reshape(1, 100)\n",
    "        q_pred = q_pred ** 2 * self.gcl\n",
    "        q_pred = torch.sum(q_pred, axis=1)\n",
    "        q_pred = (q_pred - self.E2)\n",
    "        loss_q = torch.mean(q_pred ** 2)\n",
    "        return loss_q\n",
    "        \n",
    "    def loss(self, X0, u_ics, X1):\n",
    "        if X0[0, 0] == 0:\n",
    "            L0 = 100 * self.loss_ics(X0, u_ics)\n",
    "        else:\n",
    "            L0 = self.loss_ics(X0, u_ics)\n",
    "        loss = L0 + self.loss_res(X1) + 10 * self.energy_loss(X1)    \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X).float().to(device)\n",
    "        pred_u = self.net_u(X)\n",
    "        pred_u = pred_u.cpu().detach().numpy()\n",
    "        return pred_u\n",
    "    \n",
    "    def train(self):\n",
    "        start = time.time()\n",
    "        for i in range(0, self.nt - 1):\n",
    "            X_train = self.txg[i*100 : (i + 2)*100]\n",
    "            X_init = X_train[:100]\n",
    "            X_next = X_train[-100:]\n",
    "            for epoch in range(self.epochs_Adam):\n",
    "                self.optimizer_Adam.zero_grad()\n",
    "                if i == 0:\n",
    "                    loss_value = self.loss(self.tx_ics, self.u_ics, X_train)\n",
    "                else:\n",
    "                    loss_value = self.loss(X_init, u_ics_pred, X_train)\n",
    "                if loss_value < 1e-6:\n",
    "                    print(f'i :{i}' + f' Epoch # {epoch}/{self.epochs_Adam}' + f'loss:{loss_value:.2e}')\n",
    "                    X_pred = np.hstack([np.ones((len(self.x_pred),1)) * self.t_pred[i + 1], self.x_pred.reshape(-1,1)])\n",
    "                    U_pred = self.predict(X_pred)\n",
    "                    self.pred[:, [i+1]] = U_pred\n",
    "                    break\n",
    "                else:\n",
    "                    loss_value.backward()\n",
    "                    self.optimizer_Adam.step()\n",
    "                    if (epoch+1) % self.step_size == 0:\n",
    "                        self.scheduler.step()            \n",
    "                    if epoch % 1000 == 0:\n",
    "                        if i == 0:\n",
    "                            loss_ics_value = self.loss_ics(self.tx_ics, self.u_ics)\n",
    "                        else:\n",
    "                            loss_ics_value = self.loss_ics(X_init, u_ics_pred)\n",
    "                        loss_res_value = self.loss_res(X_train)\n",
    "                        loss_energy = self.energy_loss(X_train)\n",
    "\n",
    "                        self.loss_log.append(loss_value.detach().cpu())\n",
    "                        self.loss_ics_log.append(loss_ics_value.detach().cpu())\n",
    "                        self.loss_res_log.append(loss_res_value.detach().cpu())\n",
    "                        self.loss_energy_log.append(loss_energy.detach().cpu())\n",
    "                        self.epoch_log.append(epoch)\n",
    "\n",
    "                        end = time.time()\n",
    "                        running_time = end - start\n",
    "                        start = time.time()\n",
    "\n",
    "                        print(f'Epoch #  {epoch}/{self.epochs_Adam}' + f'    time:{running_time:.2f}' + '\\n' + \\\n",
    "                              f'loss:{loss_value:.2e}, loss_ics:{loss_ics_value:.2e}, loss_res:{loss_res_value:.2e}, loss_energy:{loss_energy:.2e}')\n",
    "            u_ics_pred = self.predict(X_next)\n",
    "            u_ics_pred = torch.tensor(u_ics_pred).float().to(device)\n",
    "            #scheduler = lr_scheduler.ExponentialLR(self.optimizer_Adam, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 100)\n",
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer_Wave(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #  0/300000    time:0.58\n",
      "loss:2.58e+02, loss_ics:5.80e-01, loss_res:3.92e+01, loss_energy:7.36e-01\n",
      "Epoch #  1000/300000    time:27.46\n",
      "loss:8.77e-03, loss_ics:7.54e-05, loss_res:1.22e-03, loss_energy:2.56e-07\n",
      "Epoch #  2000/300000    time:30.74\n",
      "loss:1.24e-02, loss_ics:4.08e-05, loss_res:8.73e-03, loss_energy:3.34e-06\n",
      "Epoch #  3000/300000    time:33.37\n",
      "loss:3.54e-03, loss_ics:2.86e-05, loss_res:8.42e-04, loss_energy:2.11e-07\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f'{args.epochs_Adam}epoch,{args.step_size}step_size,{args.tol}tol,E2'\n",
    "if os.path.exists(file)==False:\n",
    "    os.mkdir(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainer.epoch_log, trainer.loss_ics_log, label='loss_ic')\n",
    "plt.plot(trainer.epoch_log, trainer.loss_res_log, label='loss_r')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f'{file}/loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = scipy.io.loadmat('data/KdV.mat')\n",
    "usol = data['uu']\n",
    "\n",
    "# Grid\n",
    "t_star = data['tt'][0]\n",
    "x_star = data['x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_star.shape)\n",
    "print(x_star.shape)\n",
    "usol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = scipy.io.loadmat('data/KdV.mat')\n",
    "usol = data['uu']\n",
    "\n",
    "# Grid\n",
    "t_star = data['tt'][0]\n",
    "x_star = data['x'][0]\n",
    "TT, XX = np.meshgrid(t_star, x_star)\n",
    "\n",
    "# Reference solution\n",
    "plt.pcolor(TT, XX, usol, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trained network parameters\n",
    "TX = np.hstack((TT.reshape(-1,1), XX.reshape(-1,1)))\n",
    "TX = torch.from_numpy(TX).double()\n",
    "\n",
    "model = trainer.model.cpu().double()\n",
    "u_pred = model(TX).detach().numpy()\n",
    "u_pred = u_pred.reshape(TT.shape)\n",
    "\n",
    "error = np.linalg.norm(u_pred - usol) / np.linalg.norm(usol) \n",
    "print('Relative l2 error: {:.3e}'.format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.pcolor(TT, XX, usol, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title(r'Exact $u(x)$')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.pcolor(TT, XX, u_pred, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title(r'Predicted $u(x)$')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.pcolor(TT, XX, np.abs(usol - u_pred), cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$x$')\n",
    "plt.title('Absolute error')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{file}/predict.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_star, usol[:,0], color='blue')\n",
    "plt.plot(x_star, u_pred[:,0], '--', color='red')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$u(t, x)$')\n",
    "plt.title('$t = 0$')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x_star, usol[:,25], color='blue')\n",
    "plt.plot(x_star, u_pred[:,25], '--', color='red')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$u(t, x)$')\n",
    "plt.title('$t = 0.5$')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x_star, usol[:,-1], color='blue')\n",
    "plt.plot(x_star, u_pred[:,-1], '--', color='red')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$u(t, x)$')\n",
    "plt.title('$t = 1.0$')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{file}/check_t.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed754970dec346c03558d641d928d07dcb8d7e3f34e484fc5786f95d70e3df3d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
